{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "# Enable eager mode. Once activated it cannot be reversed! Run just once.\n",
    "tfe.enable_eager_execution(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (60000, 28, 28, 1)\n",
      "y train (60000,)\n",
      "x test (10000, 28, 28, 1)\n",
      "y test (10000,)\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# normalization of dataset\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# flatten the dataset\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "print('x train', x_train.shape)\n",
    "print('y train', y_train.shape)\n",
    "print('x test', x_test.shape)\n",
    "print('y test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据的迭代可以重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(buffer_size=60000)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test,y_test)).shuffle(buffer_size=10000)\n",
    "'''\n",
    "show single pic with iter\n",
    "'''\n",
    "for a,b in tfe.Iterator(train_dataset):\n",
    "    print(type(a.numpy())),print(type(b.numpy()))\n",
    "    break\n",
    "\n",
    "train_dataset = train_dataset.batch(128).repeat(1)\n",
    "val_dataset = val_dataset.batch(128).repeat(1)\n",
    "# for _ in range(1000):\n",
    "#     for i,j in enumerate(tfe.Iterator(val_dataset)):\n",
    "#         print(i),print(j[0].shape,j[1].shape)\n",
    "# for i,j in enumerate(tfe.Iterator(val_dataset)):\n",
    "#     print(i),print(j['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cla(tf.keras.Model):\n",
    "    def __init__(self,num_class,checkpoint_directory):\n",
    "        super(cla, self).__init__()\n",
    "        \"\"\" Define here the layers used during the forward-pass \n",
    "            of the neural network.\n",
    "        \"\"\"   \n",
    "        # Define the checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        # Hidden layer.\n",
    "        self.conv1 = tf.layers.Conv2D(16,(3,3),activation=tf.nn.relu)\n",
    "        self.conv2 = tf.layers.Conv2D(32,(3,3),activation=tf.nn.relu)\n",
    "        self.flatten = tf.layers.Flatten()\n",
    "        self.dense_layer = tf.layers.Dense(50, activation=tf.nn.relu)\n",
    "        # Output layer. No activation.\n",
    "        self.output_layer = tf.layers.Dense(num_class, activation=None)\n",
    "        \n",
    "    def call(self,input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def loss_fn(self,input,target):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        logits = self.call(input)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=target, logits=logits)\n",
    "        return loss,logits\n",
    "    \n",
    "    def grads_fn(self,input,target):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss,_ = self.loss_fn(input, target)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\" Function to restore trained model.\n",
    "        \"\"\"\n",
    "        # Run the model once to initialize variables\n",
    "        dummy_input = tf.constant(tf.zeros((1,28,28,1)))\n",
    "        dummy_pred = self.call(dummy_input)\n",
    "        # Restore the variables of the model\n",
    "        saver = tfe.Saver(self.variables)\n",
    "        saver.restore(tf.train.latest_checkpoint\n",
    "                      (self.checkpoint_directory))\n",
    "    \n",
    "    def save_model(self, global_step=0):\n",
    "        \"\"\" Function to save trained model.\n",
    "        \"\"\"\n",
    "        tfe.Saver(self.variables).save(self.checkpoint_directory, \n",
    "                                       global_step=global_step)   \n",
    "    \n",
    "    def compute_accuracy(self, input_data):\n",
    "        \"\"\" Compute the accuracy on the input data.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            acc = tfe.metrics.Accuracy()\n",
    "            for images, targets in tfe.Iterator(input_data):\n",
    "                # Predict the probability of each class\n",
    "                logits = self.call(images)\n",
    "                # Select the class with the highest probability\n",
    "                preds = tf.argmax(logits, axis=1)\n",
    "                # Compute the accuracy\n",
    "                acc(tf.reshape(targets, [-1,]), preds)\n",
    "        return acc\n",
    "    \n",
    "    def fit(self, training_data, eval_data, optimizer, num_epochs=500, \n",
    "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. You can either train from scratch\n",
    "            or load the latest model trained. Early stopping is used in order to\n",
    "            mitigate the risk of overfitting the network.\n",
    "            \n",
    "            Args:\n",
    "                training_data: the data you would like to train the model on.\n",
    "                                Must be in the tf.data.Dataset format.\n",
    "                eval_data: the data you would like to evaluate the model on.\n",
    "                            Must be in the tf.data.Dataset format.\n",
    "                optimizer: the optimizer used during training.\n",
    "                num_epochs: the maximum number of iterations you would like to \n",
    "                            train the model.\n",
    "                early_stopping_rounds: stop training if the loss on the eval \n",
    "                                       dataset does not decrease after n epochs.\n",
    "                verbose: int. Specify how often to print the loss value of the network.\n",
    "                train_from_scratch: boolean. Whether to initialize variables of the\n",
    "                                    the last trained model or initialize them\n",
    "                                    randomly.\n",
    "        \"\"\" \n",
    "    \n",
    "        if train_from_scratch==False:\n",
    "            self.restore_model()\n",
    "        \n",
    "        # Initialize best loss. This variable will store the lowest loss on the\n",
    "        # eval dataset.\n",
    "        best_loss = np.Inf\n",
    "        \n",
    "        # Initialize classes to update the mean loss of train and eval\n",
    "         \n",
    "        \n",
    "        # Initialize dictionary to store the loss history\n",
    "        self.history = {}\n",
    "        self.history['train_loss'] = []\n",
    "        self.history['eval_loss'] = []\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['eval_acc'] = []\n",
    "        \n",
    "        # Begin training\n",
    "        train_loss = tfe.metrics.Mean()\n",
    "        eval_loss = tfe.metrics.Mean()\n",
    "        train_acc = tfe.metrics.Mean()\n",
    "        eval_acc = tfe.metrics.Mean()\n",
    "        accuracy = tfe.metrics.Accuracy()\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            # Training with gradient descent\n",
    "            traing_begin = time.time()\n",
    "            for images, target in tfe.Iterator(training_data):\n",
    "                grads = self.grads_fn(images, target)\n",
    "                optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            print('trainging\\'s time is %f s' % (time.time()-traing_begin))\n",
    "\n",
    "            # Compute the loss on the training data after one epoch\n",
    "            for images, target in tfe.Iterator(training_data):\n",
    "                loss,logits = self.loss_fn(images, target)\n",
    "                #print('train loss is %f' % loss.numpy())\n",
    "                train_loss(loss)\n",
    "                preds = tf.argmax(logits, axis=1)\n",
    "                # Compute the accuracy\n",
    "                accuracy(tf.cast(preds,tf.int32), tf.cast(target,tf.int32))\n",
    "                train_acc(accuracy.result())\n",
    "                \n",
    "            self.history['train_loss'].append(train_loss.result().numpy())\n",
    "            self.history['train_acc'].append(train_acc.result().numpy())\n",
    "            # Reset metrics\n",
    "            train_loss.init_variables()\n",
    "            accuracy.init_variables()\n",
    "            train_acc.init_variables()\n",
    "\n",
    "            # Compute the loss on the eval data after one epoch\n",
    "            for images, target in tfe.Iterator(eval_data):\n",
    "                loss,logits = self.loss_fn(images, target)\n",
    "                #print('test loss is %f' % loss.numpy())\n",
    "                preds = tf.argmax(logits, axis=1)\n",
    "                # Compute the accuracy\n",
    "                accuracy(tf.cast(preds,tf.int32), tf.cast(target,tf.int32))\n",
    "                eval_loss(loss)\n",
    "                eval_acc(accuracy.result())\n",
    "            self.history['eval_loss'].append(eval_loss.result().numpy())\n",
    "            self.history['eval_acc'].append(eval_acc.result().numpy())\n",
    "            # Reset metrics\n",
    "            eval_loss.init_variables()\n",
    "            accuracy.init_variables()\n",
    "            eval_acc.init_variables()\n",
    "\n",
    "            # Print train and eval losses\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Train loss at epoch %d: ' %(i+1), self.history['train_loss'][-1])\n",
    "                print('Eval loss at epoch %d: ' %(i+1), self.history['eval_loss'][-1])\n",
    "                print('train acc at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
    "                print('Eval acc at epoch %d: ' %(i+1), self.history['eval_acc'][-1])\n",
    "\n",
    "            # Check for early stopping\n",
    "            if self.history['eval_loss'][-1]<best_loss:\n",
    "                best_loss = self.history['eval_loss'][-1]\n",
    "                count = early_stopping_rounds\n",
    "                self.save_model(i)\n",
    "            else:\n",
    "                count -= 1\n",
    "            if count==0:\n",
    "                break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where you want to save/restore the trained variables.\n",
    "checkpoint_directory = 'models_checkpoints/myself/'\n",
    "\n",
    "# Define optimizer.\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# Instantiate model. This doesn't initialize the variables yet.\n",
    "model = cla(num_class=10,checkpoint_directory=checkpoint_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainging's time is 23.319279 s\n",
      "Train loss at epoch 1:  0.05657598376869838\n",
      "Eval loss at epoch 1:  0.05407835519577883\n",
      "train acc at epoch 1:  0.9836413701472394\n",
      "Eval acc at epoch 1:  0.9809335355266682\n",
      "trainging's time is 23.285469 s\n",
      "Train loss at epoch 2:  0.03367277329215712\n",
      "Eval loss at epoch 2:  0.046097816056512954\n",
      "train acc at epoch 2:  0.9903053613854902\n",
      "Eval acc at epoch 2:  0.9829784233248888\n",
      "trainging's time is 23.315906 s\n",
      "Train loss at epoch 3:  0.025660915919311487\n",
      "Eval loss at epoch 3:  0.04528208744131571\n",
      "train acc at epoch 3:  0.9922107704751149\n",
      "Eval acc at epoch 3:  0.9879037392339555\n",
      "trainging's time is 23.086178 s\n",
      "Train loss at epoch 4:  0.01845714428163231\n",
      "Eval loss at epoch 4:  0.047888580503390184\n",
      "train acc at epoch 4:  0.9941769316459182\n",
      "Eval acc at epoch 4:  0.9833732218655078\n",
      "trainging's time is 23.579183 s\n",
      "Train loss at epoch 5:  0.016390818323362182\n",
      "Eval loss at epoch 5:  0.04020401523477999\n",
      "train acc at epoch 5:  0.9938216521822332\n",
      "Eval acc at epoch 5:  0.9865856182374418\n",
      "trainging's time is 23.029610 s\n",
      "Train loss at epoch 6:  0.007280669469291519\n",
      "Eval loss at epoch 6:  0.04014650708590516\n",
      "train acc at epoch 6:  0.997960787608009\n",
      "Eval acc at epoch 6:  0.987506847252578\n",
      "trainging's time is 23.231180 s\n",
      "Train loss at epoch 7:  0.008180064279392465\n",
      "Eval loss at epoch 7:  0.043234207024547285\n",
      "train acc at epoch 7:  0.997107731216658\n",
      "Eval acc at epoch 7:  0.987899390445174\n",
      "trainging's time is 23.694648 s\n",
      "Train loss at epoch 8:  0.00497943556544295\n",
      "Eval loss at epoch 8:  0.04651708349945095\n",
      "train acc at epoch 8:  0.9986758951972812\n",
      "Eval acc at epoch 8:  0.9883763087690036\n",
      "trainging's time is 23.140321 s\n",
      "Train loss at epoch 9:  0.0058822451809974466\n",
      "Eval loss at epoch 9:  0.050145265413816614\n",
      "train acc at epoch 9:  0.9980485085544631\n",
      "Eval acc at epoch 9:  0.9870848777129464\n",
      "trainging's time is 23.720227 s\n",
      "Train loss at epoch 10:  0.0029872296546294324\n",
      "Eval loss at epoch 10:  0.04997731031167698\n",
      "train acc at epoch 10:  0.9988809125516088\n",
      "Eval acc at epoch 10:  0.98766791706991\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(train_dataset, val_dataset, optimizer, num_epochs=10, \n",
    "          early_stopping_rounds=5, verbose=1, train_from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
